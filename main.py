import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
from io import BytesIO
from pathlib import Path

import pandas as pd
import streamlit as st

from src import logger
from src.common.consts import ALLOWED_EXTENSIONS, ALLOWED_EXTENSIONS_WITH_ZIP, MAX_CHAR_LEN_PER_FILE, OUTPUT_DTYPE_DICT
from src.common.models import ReportFile, ReportFileList, reset_all_category_info
from src.processor.generator import Generator
from src.processor.reader import FileReader
from src.utils.google_drive import GD_DOCS_FILE_URL, GD_RESULT_FOLDER_ID, GoogleDriveHelper
from src.utils.io import excel_col_index_to_name, get_current_datetime, get_suffix, unzip_as_dict

gd_helper = GoogleDriveHelper(GD_RESULT_FOLDER_ID)


if "category_id_to_name_ko_dict" not in st.session_state:
    reset_all_category_info()


def read_report_file(file, name=None) -> ReportFile | str:
    try:
        name = file.name if name is None else name
        extension = get_suffix(name)
        file_reader = FileReader(file=file, filetype=extension, clean=True)
        return ReportFile(name=name, content=file_reader.text)
    except Exception as e:
        return f"{e.__class__.__name__}: {str(e)}"


def read_report_files_concurrently(files, names) -> dict[str, ReportFile | str]:
    report_files_dict = {}
    with ThreadPoolExecutor() as executor:
        future_to_name = {executor.submit(read_report_file, file, name): name for file, name in zip(files, names)}
        for future in as_completed(future_to_name):
            name = future_to_name[future]
            result = future.result()  # ì—¬ê¸°ì„œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸ëŠ” read_report_file í•¨ìˆ˜ ë‚´ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨
            report_files_dict[name] = result
    return report_files_dict


async def run_llm_concurrently(report_file_list, category_id):
    generator = Generator()

    tasks = []
    for report_file in report_file_list:
        tasks.append(generator.agenerate(category=category_id, input_text=report_file.content))
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results


def raise_error(msg="Error", e=Exception):
    msg = f"{msg}: {e.__class__.__name__}: {e}"
    st.error(msg)
    logger.error(msg)


# https://docs.streamlit.io/library/api-reference/utilities/st.set_page_config
st.set_page_config(
    page_title="AI ê¸°ë°˜ ë¯¸ë˜ì—­ëŸ‰ í‰ê°€ ë„êµ¬", page_icon="ğŸ§Š", layout="centered", initial_sidebar_state="auto"  # "wide",
)

# #  Hide sidebar menu
# st.markdown(
#     # [data-testid="collapsedControl"] {
#     """
#     <style>
#         section[data-testid="stSidebar"][aria-expanded="true"]{
#             display: none;
#         }
#     </style>
#     """,
#     unsafe_allow_html=True,
# )

# Configure Streamlit page and state
st.title("AI ê¸°ë°˜ ë¯¸ë˜ì—­ëŸ‰ í‰ê°€ ë„êµ¬")
st.markdown("#### ìˆ™ëª…ì—¬ëŒ€ SSK ì—°êµ¬ì‚¬ì—… AI-CALIíŒ€ ê°œë°œ")
st.write(
    "ì´ ì ìˆ˜ëŠ” ì—°êµ¬ì§„ì´ ê°œë°œí•œ ì±„ì ê¸°ì¤€ì„ í™œìš©í•˜ì—¬ GPT-4ê°€ ì±„ì ì„ ì‹œí–‰í•œ ê²°ê³¼ë¡œ, "
    + "í–¥í›„ ì±„ì ì˜ ì‹ ë¢°ë„ì™€ íƒ€ë‹¹ë„ë¥¼ í‰ê°€í•˜ê³  ê°œì„ í•˜ê¸° ìœ„í•œ ì—°êµ¬ìë£Œë¡œ í™œìš©ë©ë‹ˆë‹¤. "
    + "í˜„ì¬ ë‹¨ê³„ì—ì„œ GPT-4ì˜ ì±„ì ê²°ê³¼ëŠ” ì‹¤ì œ í•´ë‹¹ ì—­ëŸ‰ì˜ íŠ¹ì„±ì„ ì¶©ë¶„íˆ ë°˜ì˜í•˜ê³  ìˆì§€ ì•Šì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, í•´ì„ê³¼ ì‚¬ìš© ì‹œ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤"
)

with st.form("input"):
    st.markdown(f"ì—­ëŸ‰ [â„¹ï¸]({GD_DOCS_FILE_URL})", unsafe_allow_html=True)
    category_id_selected = st.selectbox(
        "ì—­ëŸ‰",
        options=tuple(st.session_state["category_id_to_name_ko_dict"].keys()),
        format_func=lambda x: st.session_state["category_id_to_name_ko_dict"][x],
        label_visibility="collapsed",
        # index=None,
        # placeholder="Select contact method...",
    )
    st.markdown(f"ê³¼ì œ íŒŒì¼ ì—…ë¡œë“œ({' '.join(ALLOWED_EXTENSIONS_WITH_ZIP)})")
    upload_files = st.file_uploader(
        "ê³¼ì œ íŒŒì¼ ì—…ë¡œë“œ",
        accept_multiple_files=True,
        type=[ext[1:] for ext in ALLOWED_EXTENSIONS_WITH_ZIP],
        label_visibility="collapsed",
    )
    # for filename, content in file_info.items():
    #     src_path = tgt_dir / filename
    #     async with aiofiles.open(src_path, "wb") as f:
    #         await f.write(content)
    #         logger.info(f"File uploaded to {src_path}")

    submitted = st.form_submit_button("í‰ê°€í•˜ê¸°")


if submitted:
    if not upload_files:
        st.error("1ê°œ ì´ìƒì˜ íŒŒì¼ì„ ì²¨ë¶€í•´ì£¼ì„¸ìš”.")
        st.stop()
    logger.info(f"File uploaded: {[file.name for file in upload_files]}")

    with st.spinner("íŒŒì¼ì„ ì½ê³  ìˆìŠµë‹ˆë‹¤..."):
        files_dict = dict()
        for upload_file in upload_files:
            suffix = get_suffix(upload_file.name)
            if suffix == ".zip":
                _files_dict = unzip_as_dict(upload_file, return_as_file=True)
                for filename, file in _files_dict.items():
                    if (ext := get_suffix(filename)) not in ALLOWED_EXTENSIONS:
                        raise_error(f"The {upload_file.name} file include unsupported file type: {ext}")

                files_dict.update(_files_dict)

            else:
                files_dict[upload_file.name] = upload_file

        input_file_dict = read_report_files_concurrently(files_dict.values(), files_dict.keys())
        error_dict = {filename: v for filename, v in input_file_dict.items() if isinstance(v, str)}
        if error_dict:
            for filename, error_msg in error_dict.items():
                error_msg = f"'{filename}'ì„ ì½ëŠ” ë„ì¤‘ ì˜¤ë¥˜({error_msg})ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                if Path(filename).suffix == ".hwp":
                    error_msg += " pdfë‚˜ word íŒŒì¼ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤"
                st.error(error_msg)
            st.stop()

        input_file_list = ReportFileList([v for v in input_file_dict.values()])
        for file in input_file_list:
            if len(file.content) > MAX_CHAR_LEN_PER_FILE:
                st.warning(
                    f"Since the '{file.name}' file is too long"
                    + f"({len(file.content)} chars), "
                    + f"only the content up to {MAX_CHAR_LEN_PER_FILE} will be used for processing."
                )
                file.content = file.content[:MAX_CHAR_LEN_PER_FILE]
        st.write(f"ì´ {len(input_file_list)}ê°œ íŒŒì¼ì„ ì½ì—ˆìŠµë‹ˆë‹¤.")
        logger.info(f"File loaded: {[file.name for file in input_file_list]}")

    with st.spinner("í‰ê°€ì¤‘ì…ë‹ˆë‹¤... ì•½ 1~2ë¶„ ì†Œìš”ë©ë‹ˆë‹¤."):
        # Run LLM
        logger.info("Start to run LLM...")
        results = asyncio.run(run_llm_concurrently(report_file_list=input_file_list, category_id=category_id_selected))
        assert len(results) == len(input_file_list)

        stu_id_base = get_current_datetime(format="%y%m%d_%H%M%S")
        total_results = []
        for idx, (report_file, result) in enumerate(zip(input_file_list, results), start=1):
            _result = {"STU ID": f"{stu_id_base}_{idx}", "ë¹„ê³ ": ""}
            if isinstance(result, Exception):
                if len(input_file_list) == 1:
                    raise_error("Error raise", result)
                    st.stop()
                else:
                    _result["ë¹„ê³ "] = result
            else:
                _result.update(result["score_info"])
                _result.update({"ì‚¬ìš© ëª¨ë¸ëª…": result["model_name"]})
            _result.update({"ì›ë¬¸íŒŒì¼ëª…": report_file.name, "ì›ë¬¸ ë‚´ìš©": report_file.content})

            total_results.append(_result)

        # ëª¨ë“  ê²°ê³¼ì— ì¼ë¶€ í‚¤ê°’ì´ ì—†ëŠ” ê²½ìš°ê°€ ìˆì„ ìˆ˜ ìˆê¸°ì—, êµ¬ì¡° ë§ì¶°ì£¼ê¸° ìœ„í•´ ë¹ˆ ë°ì´í„°í”„ë ˆì„ì„ ë¨¼ì € ìƒì„±
        # Should sync with serialize_score_info in agenerate
        criteria_dict = st.session_state["prompt_per_category_dict"][category_id_selected]
        output_dtype_dict = OUTPUT_DTYPE_DICT[0].copy()
        for crit_dict in criteria_dict["criteria"]:
            prefix = crit_dict["title_en"].lower()
            output_dtype_dict.update(
                {f"{prefix}_{sub_idx+1}": "Int64" for sub_idx in range(len(crit_dict["sub_criteria"]))}
            )
            output_dtype_dict[f"{prefix}_total"] = "Int64"
        output_dtype_dict.update({"Total": "Int64"})
        output_dtype_dict.update(
            {f"{crit_dict['title_en'].lower()}_descript": "str" for crit_dict in criteria_dict["criteria"]}
        )
        output_dtype_dict.update(OUTPUT_DTYPE_DICT[1].copy())

        result_df = pd.DataFrame(columns=output_dtype_dict.keys())
        new_df = pd.DataFrame(total_results)
        result_df = pd.concat([result_df, new_df], ignore_index=True)
        output_str_columns = [colname for colname, t in output_dtype_dict.items() if t == "str"]
        result_df.loc[:, output_str_columns] = result_df[output_str_columns].fillna("")
        result_df = result_df.astype(output_dtype_dict)

    with st.spinner("ê²°ê³¼ë¥¼ êµ¬ê¸€ë“œë¼ì´ë¸Œì— ì—…ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        # encoding = "utf-8-sig"
        # filename = f"report_{stu_id_base}.csv"
        # result_csv_bytes = result_df.to_csv(index=False).encode(encoding)
        filename = f"report_{stu_id_base}.xlsx"
        output = BytesIO()
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            result_df.to_excel(writer, index=False)

            workbook = writer.book
            worksheet = writer.sheets["Sheet1"]

            # ì¹¼ëŸ¼ ë„ˆë¹„ ì„¤ì •
            cell_format = {}  # workbook.add_format({"text_wrap": True})
            col_span_name = f"{excel_col_index_to_name(0)}:{excel_col_index_to_name(0)}"
            worksheet.set_column(col_span_name, 15, cell_format)
            long_width_col_idxs = [
                idx
                for idx, key_name in enumerate(output_dtype_dict.keys())
                if "_descript" in key_name or key_name == "ì›ë¬¸ ë‚´ìš©"
            ]
            for idx in long_width_col_idxs:
                col_span_name = f"{excel_col_index_to_name(idx)}:{excel_col_index_to_name(idx)}"
                worksheet.set_column(col_span_name, 40, cell_format)

            # ëª¨ë“  í–‰ì˜ ë†’ì´ ì„¤ì •
            row_height = 100  # ì›í•˜ëŠ” í–‰ ë†’ì´
            cell_format = workbook.add_format({"text_wrap": True, "valign": "top"})  # ìƒë‹¨ ì •ë ¬
            for row in range(len(result_df)):
                worksheet.set_row(row + 1, row_height, cell_format)  # í—¤ë” í–‰ ì œì™¸ ë‚˜ë¨¸ì§€

        result_xlsx_bytes = output.getvalue()

        # Save to google drive
        try:
            # folder = gd_helper.create_folder(stu_id_base)
            file = gd_helper.upload_byte_obj(filename, result_xlsx_bytes)  # , folder_id=folder["id"])

        except Exception as e:
            raise_error("Cannot upload result to google drive", e)
        else:
            st.link_button("ê²°ê³¼ Google driveì—ì„œ í™•ì¸í•˜ê¸°", url=file["alternateLink"])  # folder["alternateLink"])
            # embedLink: preview, webContentLink: downlaod
            # st.link_button("Google driveì—ì„œ í™•ì¸í•˜ê¸°", url=file["alternateLink"])

        # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì¶”ê°€
        # st.download_button("ê²°ê³¼ ë‹¤ìš´ë°›ê¸°", result_csv_bytes, filename, "text/csv", key="download-csv")
        st.download_button(
            "ê²°ê³¼ íŒŒì¼ë¡œ ë‹¤ìš´ë°›ê¸°",
            result_xlsx_bytes,
            filename,
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            key="download-xlsx",
        )

        st.success("í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

# st.write(f"ê¸¸ì´: {len(content)} ì")
# st.write(f"ë¹„ìš©: {cost:.3f} usd($)")
